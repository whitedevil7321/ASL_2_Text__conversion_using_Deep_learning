<!-- # requirements.txt
transformers>=4.37.0
pytorchvideo>=0.1.5
torch>=1.13.0
torchvision>=0.14.0
evaluate>=0.4.0
gradio>=3.32.0
scikit-learn
matplotlib
numpy
opencv-python -->


# README.md
# Sign Language Classification using VideoMAE

This project fine-tunes the Hugging Face [VideoMAE model](https://huggingface.co/MCG-NJU/videomae-base) to classify word-level ASL (American Sign Language) videos.

---

## ğŸ“ Project Structure
```
sign_language_videomae/
â”œâ”€â”€ dataset.py               # Custom dataset class
â”œâ”€â”€ utils.py                 # Data transforms
â”œâ”€â”€ train.py                 # Model training and evaluation
â”œâ”€â”€ split_dataset.py         # Script to split raw data into train/val/test
â”œâ”€â”€ infer.py                 # Inference class for single video
â”œâ”€â”€ predict_cli.py           # CLI prediction script
â”œâ”€â”€ export_predictions.py    # Export test predictions to CSV
â”œâ”€â”€ web_ui.py                # Gradio-based video classifier UI
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # Instructions
â””â”€â”€ data/                    # Auto-created train/val/test folders
```

---

## ğŸ“¦ Installation
```bash
git clone <this-repo>
cd sign_language_videomae
pip install -r requirements.txt
```

---

## ğŸ§¾ Dataset Structure
Your raw dataset should be located at:
```
D:/Downloads/archive/dataset/SL/
```
And look like:
```
SL/
â”œâ”€â”€ Hello/
â”œâ”€â”€ Thanks/
â”œâ”€â”€ ... (239 classes)
```

---

## ğŸš€ Training
```bash
python train.py
```
- Automatically splits dataset into train/val/test (80/10/10).
- Trains VideoMAE on your sign classes.
- Saves model, config, confusion matrix, and labels.txt in a timestamped folder.

---

## ğŸ§ª Inference Options
### 1. Python API
```python
from infer import SignPredictor
predictor = SignPredictor("videomae-sign-YYYYMMDD-HHMMSS")
result = predictor.predict("path/to/video.mp4", id2label)
print(result)
```

### 2. CLI Tool
```bash
python predict_cli.py --video path/to/video.mp4 \
  --model videomae-sign-YYYYMMDD-HHMMSS \
  --labels "Hello,Thanks,Yes,..."
```

### 3. Web App
```bash
python web_ui.py
```
Opens a Gradio UI in your browser.

### 4. Export Test Predictions
```python
from export_predictions import BatchPredictor
predictor = BatchPredictor("videomae-sign-YYYYMMDD-HHMMSS", label2id)
predictor.export_to_csv("data/test")
```

---

## âœ… Output
After training, the model folder will contain:
```
videomae-sign-YYYYMMDD-HHMMSS/
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ config.json
â”œâ”€â”€ preprocessor_config.json
â”œâ”€â”€ labels.txt
â”œâ”€â”€ confusion_matrix.png
```

---

## ğŸ“Œ Notes
- Videos should be short (1â€“2 seconds, ~30â€“60 frames).
- Uses 16 uniformly sampled frames for training.
- No Mediapipe required â€” full end-to-end using raw video.

---

## ğŸ™Œ Credits
- [VideoMAE on Hugging Face](https://huggingface.co/MCG-NJU/videomae-base)
- [PyTorchVideo](https://pytorchvideo.org)
- Hugging Face Transformers + Gradio
